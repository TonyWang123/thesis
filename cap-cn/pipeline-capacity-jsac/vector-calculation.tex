\section{Transmission and capacity vector calculation}
\label{sec:v-calculation}

\subsection{Transmission vector calculation}
\label{subsec:tv-calculation}
In section~\ref{subsec:function-transmission}, we defined the notion of a tv. In this section, we show how it can be computed. This computation, at a high level, is a three step process:
\begin{itemize}
  \item First, we convert a given $f$ from its HLL source code to an intermediate representation (IR).

  \item Second, we convert this IR to a data flow graph (dfg).

  \item Finally, we generate $f$'s tv from this dfg by finding min-vertex-cuts in $G$ separating each $S \in M$ from $f$'s outputs.
\end{itemize}

\para{Intermediate representation}

\para{Data flow graph}

\para{Transmission vector generation:} Now that we have defined the notion of a dfg and shown how a dfg can be calculated from a $f$, we present our \textsc{Transmission Vector Generation Theorem} (TVGT), which describes how a dfg can be used to calculate the value of $f$'s tv for a given $S$.

\vspace{2mm}
\noindent \textsc{Transmission Vector Generation Theorem:} If $f$'s dfg is $G$, then $\forall\ S \in M$: $tv[S] = G.\textit{min-vertex-cut}(V_S, D_{NS})$, where $V_S$ is the set of $G$'s roots corresponding to $S$ and $D_{NS}$ is the set of $G$'s vertices not solely descended from roots $\in V_S$ (Fig~\ref{fig:x}).
\vspace{2mm}

\noindent \textit{Proof:} We prove our TVGT using the following lemmas:

\vspace{2mm}
\noindent \textsc{Lemma 4:} Given a vertex cut disconnecting $D_{NS}$ from $V_S$ (that may cut $v \in V_S$), we can calculate $f$ without knowing $S$ if we know the bindings for the cut's vertices $C$.
\vspace{2mm}

\noindent \textit{Proof:}  We can calculate a dfg $G$'s output given bindings for G's roots because every vertex's variable in G must be descended from some subset of these roots. 

Consider the sub-graph of $G$, $G_s$ generated by removing every vertex separated from $D_{NS}$ by our vertex cut. Each of $G_s$'s roots is either a vertex corresponding to some $m_j \in M - S$ or $v \in C$. Therefore, given bindings for every vertex in $C$ and $m_j \in M - S$, we can calculate $G_s$'s output, and therefore $G$'s output. By the definition of a dfg $G$'s output is $f$'s output, and therefore we have shown that we can calculate $f$ given bindings for $C$ in lieu of bindings for $S$.

\vspace{2mm}
\noindent \textsc{Lemma 5:} If we can calculate $f$ by transmitting bindings for a set of variables $v \in V$ calculated using $S$ in lieu of bindings for $S$, $S$ has no more equivalence classes than $C$.
\vspace{2mm}

\noindent \textit{Proof:} Suppose, by way of contradiction, that a given $S$ had more equivalence classes than some $C$ calculated from it. Each of $S$'s equivalence classes' bindings must generate a binding from at least one of $C$'s equivalence classes. Therefore, by the pigeonhole principle, if $S$ has more equivalence classes than $C$, two bindings in different equivalence classes in $S$ must generate bindings from the same equivalence class in $C$. However, given that $f$'s output is agnostic to which binding in an equivalence class $C$ takes, the two bindings of $S$ must be in the same equivalence class which is a contradiction.
\vspace{2mm}

\noindent Given these lemmas, we prove TVGT as follows:
\vspace{2mm}

\noindent \textit{Proof:}
By \textsc{Lemma 4}, we can transmit a binding for the set of vertices in the cut $G.\textit{min-vertex-cut}(V_S, D_{NS})$ $C$ in lieu of a binding for $S$ to an executor and still calculate $f$. By, \textsc{Lemma 5}, $S$ can have no more vertices than $C$. We can put an upper bound on $C$'s number of equivalence classes by assuming that each of $C$'s bindings is a unique equivalence class. Since $C$  has $\sum_{v \in V}(|v.domain|)$ possible bindings, $S$ has at most $\sum_{v \in V}(|v.domain|)$ equivalence classes.

\subsection{Capacity vector calculation}
\label{subsec:tv-calculation}
In the previous sub-section we showed how a function tv could be computed. In this sub-section we proceed to show how a pipeline path cv can be computed. As before, this computation consists of three high level steps.

\begin{itemize}
  \item First, we enumerate all possible paths through a given $p$.

  \item Second, we convert each path into a pipeline dfg $G$.

  \item Finally, we generate $p$'s paths' cv from $G$ by counting the equivalence classes that can reach subgraphs of $G$.
\end{itemize}

\para{Enumerating pipeline paths}

\para{Pipeline path dfgs:} Now that we have developed a mechanism to enumerate a given $p$'s paths, we describe how we convert each of these paths $r_p$ into a dfg $G$.

Before converting $r_p$ into a $G$, we transform $r_p$ into a path $r_s$ which computes exactly the same set of functions as $r_p$ by applying our \textsc{Table Split Transformation}.

\vspace{2mm}
\textsc{Table Split Transformation:} For each table $t_p \in r_p$, if $t_p$ has multiple outputs (registers or pipeline outputs) we split $t$ into a set of tables$T_s$ that write each of $t_p$'s outputs separately and share $t_p$'s inputs, which $r_s$ visits in arbitrary order.
\vspace{2mm}

To see that splitting a $t_p$ preserves the set of $f$ that it can execute, observe that  we can represent any given $t_p$'s contents in $t_s \in T_s$ by adding each of $t_p$'s rules whose outputs include a given $t_s$'s output to that $t_s$ and vice versa, and that we can traverse $T_s$ in arbitrary order because every $t_s$ shares $t_p$'s inputs, eliminating read-write conflicts.

Given our transformation of each $r_p \in p$, we now describe how we convert each transformed path $r_s$ into a pipeline dfg. We begin by describing our dfg, below:

\vspace{2mm}
\noindent \textsc{Pipeline dfg:} A pipeline dfg is a connected dag $G = (V, E)$ with exactly one leaf in which each $v \in V$ either represents a pipeline input, a register and its $t \in r_s$, or a pipeline output and its $t \in r_s$, distinguished by $v$'s field $v.type$ taking the values $M$, $R$, and $O$ respectively, where:
\begin{itemize}

 \item Pipeline input verticies ($v.type = M$) have a $v.name$ field which stores the pipeline input that $v$ represents. Iff a $v \in V$ is a pipeline input, it is a root of $G$.

 \item Register verticies ($v.type = R$) have fields $v.name$, $v.size$, and $v.S$, which store the register's name, its bit length, and the input verticies $v$ is descended from.

 \item Pipeline output verticies ($v.type = O$) have the field $v.S$, like register verticies, and are $G$'s only leaf.

\end{itemize}

\noindent Each edge $e$ in $G$ denotes that $e$'s target $v$'s table takes $e$'s source $v$'s pipeline input or register as an input.
\vspace{2mm}

\noindent \textit{Pipeline dfg generation:} To generate a pipeline dfg $G$ from a $r_s$, we simply step through $\forall\ t \in r_s$ in packet traversal order, adding $M$ verticies to $G$ for each unseen pipeline input $\in t$'s inputs, and a $R$ or $O$ vertex to $G$ for each $t$'s output, and edges between each $t$'s vertex and the verticies $t$ reads.

After generating $G$, we enumerate each $M$ vertex $v_m$'s descendents $d \in D$, and add $v_m.name$ to $d.S$.

\para{Capacity vector generation:} Now that we have defined our pipeline path dfg and shown how such a dfg $G$ can be computed, we present our \textsc{Capacity Vector Generation Theorem}, which describes how $G$ is used to find a path's cv.


\vspace{2mm}
\noindent \textsc{Capacity Vector Generation Theorem:} $\forall$ vertex $v \in G$, if $v.type = R$ or $O$, we add $cv[v.S] = v.size$ to our cv. If $cv[v.S]$ already exists, we update its value if $cv[v.S] > v.size$.
\vspace{2mm}

\noindent \textit{Proof:} We prove our CVGT using the following lemma:
\vspace{2mm}

\noindent \textsc{Lemma 6:} Given a $v \in G$ with parents $p \in P$, if $v.type \neq M$, $v$'s table $t$ can compute any $f$ with inputs $m_f \in M_f$ which:
\begin{itemize}
 \item Only takes inputs that appear in at least one $p.S$. 
 \item $f$ only has $2^{p.size}$ equivalence classes $\forall\ p.S \cap M_f$.
\end{itemize}
\vspace{2mm}

\noindent \textit{Proof:} A table $t$ is an executor capable of running arbitrary computation on its inputs. Any $t$, therefore, can execute any $f$ if $t$ receives sufficient information about each $m_f \in M_f$.

A $t$ can only receive such information from its parents $p \in P$, each of which is capable of transmiting any combination of information about the pipeline inputs $p.S$ that $p$ is descended from. By \textsc{Lemma 1}, if $f$ has less than $2^{p.size}$ equivalence classes for a given $p.S$, $t$ requires at most $p.size$ bits about $p.S$ to execute $f$, and therefore $t$ is capable of receiving sufficient information from $p$ about $p.S$ to execute $f$.

Therefore, if every $m_f \in M_f$ appears in at least one $p.S$, and $f$ has less than $2^{p.size}$ equivalence classes for every $p.S$, $t$ can receive sufficient information about every $m_f \in M_f$ to execute $f$ from at least one $p$, and thus $t$ can execute $f$.

\vspace{5mm}

We can parameterize a path through a pipeline as follows:
outputs = table[inputs]

We can view a table, in a vaccuum, as follows.

A table has a set of input in inputs.

Each input in a table's inputs has two properties
- the pipeline inputs it can carry information about
- the number of bits of information it carries

Tables have two types of input: registers, and pipeline inputs.

Pipeline inputs mp carry information about a single input, mp, but can carry an arbitrary amount of information about those inputs. 

Registers can carry information about any input they are descended form in the pipeline dfg, and can carry their bit length bits about these inputs.

Note, importantly, that we do not assume any specific structure in the information that a register can carry about an input - if a k-bit register is descended from inputs (mi, mj, mk), we assume that this register can carry k bits of information about mi, or mj, or mk, or any combination of these inputs. Depending on the nature of the pipeline previously, this, of course, may not be true, but we assume that it is.

Our capacity vector is simply:
- for each unique S in Mp, if RS, the set of all registers carrying S, is non-zero, we add the term cv[S] = rs.size.

The pipeline capacity theorem: Given an f with inputs mf in Mf and rs with inputs mr in Mr, if Mf is a subset of Mp and tv[S] < cv[S] for all S in Mf, then we can execute f in rs.

We prove our theorem by induction. First, we will show that our theorem is true for the last table in rs, and then we will inductively prove that if our theorem is true for the last k tables in rs, it is true for the last k+1 tables in rs.

Base case:
- Consider the last table in rs, l.
- We can think of l as an executor which can compute an arbitrary output from its inputs i.
- By Lemma 2, therefore, if l receives tv[S] bits of information from its i about a set of S in Mf such that each mf in Mf appears in at least one S, l can compute f.
- For each i in I, if i is a pipeline input it can carry arbitrary information about 


- the inductive step must show that each i that is a register can transmit arbitrary information about I. 


- l has two types of inputs, registers and pipeline inputs.
- if 



Base case:
- Consider a  consisting only of a pipeline's output table, o.


Base case:
- Consider a dfg consisting only of a pipeline's output table, o. 
- The cv of this table is cv[p.S] = p.size for all p in P, o's inputs.
- By lemma 2, any f can be computed by an executor that receives tv[S] bits
  about a set of S such that all mf in Mf are in at least 1 S.
- By the definition of a cv, o receives cv[p.S] bits about every p.S.
- Therefore, if tv[S] < cv[p.S] for all p, and all mf in Mf are in at least one p.S, o can calculate f, which is our inductive hypothesis.

Inductive case:
- Suppose we have a dfg consisting of an arbitrary set of tables  


- By Lemma 2, f can be computed by any executor receives tv[S] bits of data about a set of Ss such that all mf in Mf are in at least 1 S.



For example, in t2,

Example:
r1 = t1[m1, m2]
r2 = t2[r1, m3]
r3 = t3[r1, m4]
eg = t4[r2, r3]

Each table


