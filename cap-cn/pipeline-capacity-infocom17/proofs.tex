We now present proofs to verify our main results. The structure of these proofs will be as follows. First, we define a mechanism to encode sufficient information about a given $M \in \mathcal{M}$ to fully execute a given $f$. Second, we show that a pipeline transmitting information internally using our encoding can realize an $f$ in $p$ given that $\kappa(p) \trianglerighteq \tau_G(f)$. Finally, we show that $\tau_G(f) > \tau(f)$, proving by extension that if $\kappa(p) \trianglerighteq \tau(f), f \rightrightharpoons p$. We omit a proof of Theorem 2 and the proofs of certain corollaries and lemmas due to space constraints. We will give these proofs in an extended report in an upcoming technical journal.

We base our summary on the vertices in the $G_f.vertexMinCut(M)$ of a $f$'s $G_f$. %We start defining notation around these vertices.

\begin{definition}
The {\em min cut vertices $\mu_f(M)$} are the vertices in an $f$'s $G_f$ cut by $G_f.vertexMinCut(M).$ 
\end{definition}

Let a given value of $\mu_f(M)$ be $v_i(\mu_f(M))$ and the domain of values of $\mu_f(M)$ be $dom(\mu_f(M))$. %We now prove the fundamental property of this set that allows us to use it as a representation:

\begin{lemma}
\label{lemma:M-to-mu}
Given a $G_f.vertexMinCut(M)$, we can calculate $f$ without knowing $v_i(M)$ given $v_i(\mu_f(M))$.
\end{lemma}

%\begin{proof}
%We can calculate any DFG $G$'s output given the values of all of its roots because every vertex in $G$ must be descended from a subset of these roots.

%Consider the subgraph of $G_f$, $G_{f,M}$, generated by removing every vertex in $G_f$ that $\mu_f(M)$ separates from $D_f(M)$. By our definition of $G_f$, $G_f$'s output node is descended from $\forall\ m_i \in \mathcal{M}$, and thus it is always in $G_{f,M}$.

%Each of $G_{f,M}$'s roots is either a $m_j \in \mathcal{M} - M$ or some vertex in $\mu_f(M)$. Therefore, given $v_i(\mathcal{M} - M)$ and $v_j(\mu_f(M))$, we can calculate $G_{f,M}$'s output, and therefore $G_f$'s output. By the definition of a DFG, $G_f$'s output is $f$'s output, and therefore we have shown that we can calculate $f$ given $v_j(\mu_f(M))$ in lieu of $v_j(M)$.
%\end{proof}

%Further, we show that this representation is a bound on the number of equivalence classes of $M$.

%\begin{proof}
%By Lemma~\ref{lemma:domM-domMu}, $|dom(M)/\sim_f| \leq |dom(\mu_f(M))/\sim_f|$. Further, $|dom(\mu_f(M))\sim_f| < |dom(\mu_f(M))|$. Finally, $|dom(\mu_f(M))| < \prod_{\mu_f_i \in \mu_f(M)} dom(\mu_f_i)$, which is the value of $G_f.vertexMinCut(M)$.
%\end{proof}

While $\mu_f(M)$ acts as an effective representation of values of $M$ $v_i(M)$, we can compress it by introducing the concept of codewords, allowing us to maximize transmission through a pipeline.

\begin{definition} The {\em codewords $\chi_f(M)$} of inputs $M$ of a $f$ are a set of integers that correspond to the f-equivalence classes of $M$.
\end{definition}

Receiving a codeword $\in \chi_f(M)$ is equivalent to receiving a value for $M$ $v_i(M)$, since the codeword can be deterministically mapped back into a value from $v_i(M)$'s equivalence class. We now define the shorthand `\textit{compute the codewords of $M$}' which we will use in our proofs:

\begin{definition} If we can {\em compute the codewords $\chi_f(M)$} of $M$, $\forall\ v_i(M) \in dom(M)$ we can compute the codeword associated with the equivalence class of $v_i(M)$.
\end{definition}

Our codewords give us a bound on the transmission requirements of an $M$, given in Lemma 2.

\begin{lemma} 
\label{lemma:codewords}
A table $t_i$ only requires $log_2(\lceil |dom(\mu_f(M))|-1 \rceil)$ bits of information about $M$ to execute $f$ correctly.
\end{lemma}

\begin{proof}
We can encode the value of any $v_i(M) \in dom(M)$ as a codeword in $\chi_f(M)$ and still convey sufficient information to compute $f$. If $\mu_f(M)$ can take $|dom(\mu_f(M))|$ distinct values, we can assign each value a unique codeword from the set [$0$, $...$, $|dom(\mu_f(M))|-1$], which take at most $log_2(\lceil |dom(\mu_f(M))|-1 \rceil)$ bits to represent.
\end{proof}

\para{Proving the realization theorem:} Given our characterization of function transmission requirements, we can now embark on our proof of our realization theorem. First, we will give our key underlying lemma, lemma 3, from which our realization theorem follows naturally.

\begin{lemma} 
\label{lemma:core-lemma}
If $\forall\ t_i \in \rho = \langle t_1, ..., t_n \rangle$ have $maxRules(t_i) > \tau_G(f)(\bar{M}_\rho(t_i))[dom]$, and $2^{r(t_i)} > \tau_G(f)(\bar{M}_\rho(t_i))[ec]$, then $\forall t_i \in \rho = \langle t_1, ..., t_n \rangle$ can output $\chi_f(\bar{M}_\rho(t_i))$ to $r(t_i)$.
\end{lemma}
%
%\begin{proof}
%\noindent We prove Lemma~\ref{lemma:core-lemma} by induction. Due to space constraints, we omit the base case.
%\vspace{1mm}
%
%\noindent \textit{Inductive step:} Assume Lemma~\ref{lemma:core-lemma} is true for $\langle t_1, ..., t_k \rangle$. We show Lemma~\ref{lemma:core-lemma} is true for $\langle t_1, ..., t_{k+1} \rangle$.
%\vspace{1mm}
%
%We prove the inductive case in two stages. First, we show that if $maxRules(t_1) > \tau_G(f)(\bar{M}_\rho(t_1))[dom]$, $t_{1}$ can compute $\chi_f(\bar{M}_\rho(t_{1}))$. Second, we show that given that $t_{1}$ can compute $\chi_f(\bar{M}_\rho(t_{1}))$, if $2^{r(t_{1})} > \tau_G(f)(\bar{M}_\rho(t_{1}))[ec]$, $\chi_f(\bar{M}_\rho(t_{1}))$ can be output by $t_1$ to $r(t_{1})$.
%%We prove the inductive step with the same two stages we used to prove the base case.
%\vspace{1mm}
%
%We start by proving stage 1. $m_i \in \bar{M}_\rho(t_{k+1}) \Leftrightarrow m_i \in I(t_{k+1}) \vee m_i \in \bar{M}_\rho(t_{i}) : r(t_i) \in I(t_{k+1})$. $r(t_i) \in I(t_{k+1}) \Leftrightarrow r(t_i) \in (r(t_1), ..., r(t_{k+1}))$.
%
%By the inductive hypothesis, $r(t_i) \in (r(t_1), ..., r(t_{k+1})) \Rightarrow r(t_i)$ will contain $\chi_f(\bar{M}_\rho(t_{i}))$. Therefore, $t_{k+1}$ can read $\forall\ m_i \in \bar{M}_\rho(t_{k+1})$ from $m_i \in I(t_{k+1}) \vee r(t_i) \in I(t_{k+1})$.
%\vspace{1mm}
%
%Now, as in the base case, if a table $t_m$ is given $\forall\ m_i \in \bar{M}_\rho(t_{k+1})$, it can compute $\chi_f(\bar{M}_\rho(t_{k+1}))$ by matching on all $v_i(\bar{M}_\rho(t_{k+1})) \in dom(\bar{M}_\rho(t_{k+1}))$ if $maxRules(t_{m}) > dom(\bar{M}_\rho(t_{k+1}))$.
%\vspace{1mm}
%
%We now show how to transform $t_m$ into $t_{k+1}$ without increasing $t_m$'s rule number. $\forall\ m_i \in \bar{M}_\rho(t_{k+1})$:
%
%\begin{itemize}
% \item If $m_i \in I(t_{k+1})$, we can leave $m_i$'s column in $t_m$ alone.
%
% \item If $m_i \notin I(t_{k+1}) \Rightarrow r(t_i) \in I(t_{k+1}) : r(t_i)$ contains $\chi_f(M_j) : m_i \in M_j$. Further, $M_j \subseteq \bar{M}_\rho(t_{k+1})$. Thus, each rule in $t_m$ generates precisely one codeword in $\chi_f(M_j)$. We therefore replace $t_m$'s match field header $m_i$ with $\chi_f(M_j)$, and that each of that header's values in $t_m$'s rules with that rule's codeword in $\chi_f(M_j)$.
%\end{itemize}
%
%This transformation does not increase rule number and results in a table that only matches on headers in $I(t_k)$. Thus we have proved stage 1.
%
%%Thus we have shown $t_{k+1}$ can compute $\chi_f(\bar{M}_\rho(t_{k+1}))$ if $maxRules(t_{k+1}) > dom(\bar{M}_\rho(t_{k+1}))$, and $\tau_G(f)(\bar{M}_\rho(t_{k+1}))[dom] \leq dom(\bar{M}_\rho(t_{k+1}))$. 
%
%We now proceed to proving stage 2. $\chi_f(M)$ only requires $\lceil log_2(|M/\sim_f|) \rceil$ bits to represent it. Therefore $2^{r(t_{k+1})} > |\bar{M}_\rho(t_{k+1})/\sim_f| \Rightarrow \chi_f(\bar{M}_\rho(t_{k+1}))$ can be placed in $r(t_{k+1})$. $\tau_G(f)(\bar{M}_\rho(t_{k+1}))[ec] \leq 2^{r(t_{k+1})}$.
%\end{proof}

Given Lemma~\ref{lemma:core-lemma}, we are now equipped to prove the realization theorem.

\begin{proof}
Given an $f$ and $p$, we will prove that if $\kappa(p)\trianglerighteq tau(f)$, $f \rightrightharpoons p$. Consider a $\kappa_\rho(\rho) \in \kappa(p)$.

$\forall\ M \in \mathcal{M} : m_i \in M \rightarrow m_i \notin \bigcup_{t_i \in \rho} \bar{M}_\rho(t_i),\ \kappa_\rho(\rho)(M) = (1,\ 1)$. Therefore, if $\kappa_\rho(\rho) > \tau_G(f) \Rightarrow$ all $m_i$ not read by $\rho$ are treated as constants or not read at all by $f$, and thus $f$ is effectively a mapping from $\bigcup_{t_i \in \rho} \bar{M}_\rho(t_i) \rightarrow \mathcal{R}$. 

Further, given $\kappa_\rho(\rho) > \tau_G(f)$ $\forall\ t_i \in \rho,\ maxRules(t_i) > \tau_G(f)(\bar{M}_\rho(t_i))[dom]$, and $2^{r(t_i)} > \tau_G(f)(\bar{M}_\rho(t_i))[ec]$, and thus by Lemma~\ref{lemma:core-lemma} $t_n$ can calculate $\chi_f(\bar{M}_\rho(t_n))$.

Finally, consider that if a $t_i$ can calculate $\chi_f(M_i)$, and an $f$ is a mapping $dom(M_i) \rightarrow \mathcal{R}$, $t_i$ can compute $f$'s output $\forall\ v_j(M_i) \in dom(M_i)$ by mapping each codeword in $\chi_f(M_i)$ to the output of $f$ it corresponds to.

Since $t_n$ is $\rho$'s only output, $\bar{M}_\rho(t_n) = \bigcup_{t_i \in \rho} \bar{M}_\rho(t_i)$. Thus, $t_n$ can compute $f$'s output. Further, since $t_n$ is an egress table it can always pass this output back to the switch.

Therefore, if $\kappa_\rho(\rho) > \tau_G(f), f \rightrightharpoons \rho$. Since $\kappa_\rho(\rho) \in k(p)$ and $\rho \in p$, we have proved that if $\kappa(p)\trianglerighteq \tau_G(f)$, $f \rightrightharpoons p$.
\end{proof}

The last step required to prove our realization theorem is to show that $\tau_G(f) > \tau(f)$ and thus that $\kappa(p)\trianglerighteq \tau(f) \Rightarrow f \rightrightharpoons p$. The crux of this step is given in Lemma 4, below.
 
\begin{lemma} The number of equivalence classes of $M$ is bounded by $dom(\mu_f(M))$.
%$|dom(M)/\sim_f| \leq |dom(\mu_f(M))\sim_f|$.
\label{lemma:domM-domMu}
\end{lemma}

\begin{proof}
Suppose, by way of contradiction, $\exists\ (f,\ M) : |dom(M)/\sim_f| > dom(\mu_f(M)$. Each $v_i(M)$ in one of $M$'s equivalence classes must generate a $v_i(\mu_f(M))$. By the pigeonhole principle, if $M$ has more equivalence classes than $\mu_f(M)$, two values of $M$ from different equivalence classes must generate the same value of $\mu_f(M)$. However, by Lemma 1, $\mu_f(M)$ contains sufficient information about $M$ to fix $f$'s outputs value, and thus these two bindings of $M$ must be in the same equivalence class, which is a contradiction.
\end{proof}

%As a corrollary, we have shown that $G_f.vertexMinCut(M)$ bounds $M$'s equivalence classes.

\begin{corrollary} The number of f-equivalence classes of any $M$ is bounded by $G_f.vertexMinCut(M).$
\label{lemma:eq-cl-approx}
\end{corrollary}

\begin{corrollary} The characteristic function $\tau_G(f)$ dominates the characteristic function $\tau(f)$.
\label{lemma:eq-cl-approx}
\end{corrollary}

We have therefore proven our realization theorem: that $\kappa(p)\trianglerighteq \tau_G(f) \Rightarrow f \rightrightharpoons p$.
